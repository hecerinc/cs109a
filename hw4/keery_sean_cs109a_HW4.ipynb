{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Homework 4\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, October 5th, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW4.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keery, Sean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS 109a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[First and Land names of students with whom you have collaborated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Variable selection and regularization\n",
    "\n",
    "The data set for this problem is provided in ``dataset_1.txt`` and contains 10 predictors and a response variable.\n",
    "\n",
    "### Part (a): Analyze correlation among predictors\n",
    "- By visually inspecting the data set, do find that some of the predictors are correlated amongst themselves?\n",
    "\n",
    "\n",
    "- Compute the cofficient of correlation between each pair of predictors, and visualize the matrix of correlation coefficients using a heat map. Do the predictors fall naturally into groups based on the correlation values?\n",
    "\n",
    "\n",
    "- If you were asked to select a minimal subset of predictors based on the correlation information in order to build a good regression model, how many predictors will you pick, and which ones will you choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual inspection\n",
    "\n",
    "I opened the text file in Excel and noticed that the values for the first three predictors are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAF6CAYAAAAJaaMjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJgy6JDXx/wEhGltPg0jMVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11625db10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load data\n",
    "data=np.loadtxt('./datasets/dataset_1.txt',delimiter=',',skiprows=1)\n",
    "\n",
    "#split predictors and response\n",
    "x=data[:,:-1]\n",
    "y=data[:,-1]\n",
    "\n",
    "#compute matrix of correlation coefficients\n",
    "corr_matrix=np.corrcoef(x.T)\n",
    "corr_matrix.shape\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.pcolor(corr_matrix)\n",
    "\n",
    "ax.set_title('Heatmap of correlation matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the predictors fall naturally into groups based on the correlation values?\n",
    "\n",
    "There seems to be five groups.\n",
    "\n",
    "### How many predictors will you pick? \n",
    "\n",
    "Five.\n",
    "\n",
    "### Which ones will you choose?\n",
    "\n",
    "I would pick one from each group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Selecting minimal subset of predictors\n",
    "\n",
    "- Apply the variable selection methods discussed in class to choose a minimal subset of predictors that yield high prediction accuracy:\n",
    "    \n",
    "    - Exhaustive search\n",
    "    \n",
    "    - Step-wise forward selection **or** Step-wise backward selection  \n",
    "\n",
    "&emsp;&nbsp;&nbsp; In each method, use the Bayesian Information Criterion (BIC) to choose the subset size.\n",
    "\n",
    "- Do the chosen subsets match the ones you picked using the correlation matrix you had visualized in Part (a)?\n",
    "\n",
    "**Note**: You may use the `statsmodels`'s `OLS` module to fit a linear regression model and evaluate BIC. You may **not** use library functions that implement variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset by exhaustive search:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Exhaustive search\n",
    "best_subset = [] \n",
    "# predictor_set=set(range(10))\n",
    "predictor_set = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "min_bic = 1e10 # set some initial large value for min BIC score\n",
    "#outer loop\n",
    "for k in range(10):\n",
    "    # subsets for size k\n",
    "    subsets_k =it.combinations(predictor_set,k+1)\n",
    "    #inner loop to compare BIC within subsets\n",
    "    for subset in subsets_k:\n",
    "        x_subset = x[:, subset]\n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        #compute BIC\n",
    "        bic = results.bic\n",
    "#         print r_squared,bic,subset[:]\n",
    "        # keep track of best one\n",
    "        # Update max R^2 and best predictor subset of size k\n",
    "        # If current predictor subset has a higher R^2 score than that of the best subset \n",
    "        # we've found so far, remember the current predictor subset as the best!\n",
    "        if(bic < min_bic): \n",
    "            min_bic = bic\n",
    "            best_subset = subset[:]\n",
    "    \n",
    "print('Best subset by exhaustive search:')\n",
    "print sorted(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise forward subset selection:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "### Step-wise Forward Selection\n",
    "d = x.shape[1] # total no. of predictors\n",
    "\n",
    "# Keep track of current set of chosen predictors, and the remaining set of predictors\n",
    "current_predictors = [] \n",
    "remaining_predictors = range(d)\n",
    "\n",
    "# Set some initial large value for min BIC score for all possible subsets\n",
    "global_min_bic = 1e10 \n",
    "\n",
    "# Keep track of the best subset of predictors\n",
    "best_subset = [] \n",
    "\n",
    "# Iterate over all possible subset sizes, 0 predictors to d predictors\n",
    "for size in range(d):    \n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "    best_predictor = -1 # set some throwaway initial number for the best predictor to add\n",
    "    bic_with_best_predictor = 1e10 # set some initial large value for BIC score   \n",
    "        \n",
    "    # Iterate over all remaining predictors to find best predictor to add\n",
    "    for i in remaining_predictors:\n",
    "        # Make copy of current set of predictors\n",
    "        temp = current_predictors[:]\n",
    "        # Add predictor 'i'\n",
    "        temp.append(i)\n",
    "                                    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, temp]\n",
    "        \n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "        if(r_squared > max_r_squared):\n",
    "            max_r_squared = r_squared\n",
    "            best_predictor = i\n",
    "            bic_with_best_predictor = results.bic\n",
    "    \n",
    "    # Remove best predictor from remaining list, and add best predictor to current list\n",
    "    remaining_predictors.remove(best_predictor)\n",
    "    current_predictors.append(best_predictor)\n",
    "    \n",
    "    # Check if BIC for with the predictor we just added is lower than \n",
    "    # the global minimum across all subset of predictors\n",
    "    if(bic_with_best_predictor < global_min_bic):\n",
    "        best_subset = current_predictors[:]\n",
    "        global_min_bic = bic_with_best_predictor\n",
    "    \n",
    "print 'Step-wise forward subset selection:'\n",
    "print sorted(best_subset) # add 1 as indices start from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The predictors match the ones I selected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Lasso and Ridge regression\n",
    "\n",
    "- Apply Lasso regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Apply Ridge regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Is there a difference between the model parameters you obtain different and those obtained from Lasso regression? If so, explain why.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Is there anything peculiar that you observe about the coefficients Ridge regression assigns to the first three predictors? Do you observe the same with Lasso regression? Give an explanation for your observation.\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to perform Lasso and Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:\n",
      "Coefficients: [ 0.02717417  0.          0.         -0.         -0.02532806 -0.         -0.\n",
      "  0.04397321 -0.40612185 -0.22260474]\n",
      "Predictors that are assigned non-zero coefficients are: [0, 4, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#Apply Lasso regression with regularization parameter  λ=0.01 and fit a regression model.\n",
    "reg = Lasso_Reg(alpha = 0.01)\n",
    "reg.fit(x, y)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "print 'Lasso:'\n",
    "print 'Coefficients:', coefficients\n",
    "print  'Predictors that are assigned non-zero coefficients are:', [i for i, item in enumerate(coefficients) if abs(item) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge:\n",
      "Coefficients: [ 0.04353543  0.04353543  0.04353543  0.55217415 -0.19706852 -0.61421737\n",
      "  0.30484213  0.18742866 -0.50083242 -0.35908145]\n",
      "Predictors that are assigned non-zero coefficients are: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#Apply Ridge regression with regularization parameter  λ=0.01 and fit a regression model.\n",
    "reg = Ridge_Reg(alpha = 0.01)\n",
    "reg.fit(x, y)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "print 'Ridge:'\n",
    "print 'Coefficients:', coefficients\n",
    "print  'Predictors that are assigned non-zero coefficients are:', [i for i, item in enumerate(coefficients) if abs(item) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there a difference between the model parameters you obtain different and those obtained from Lasso regression? If so, explain why.\n",
    "\n",
    "No, they do not correspond.  The lasso regression shows that all the parameters should be included in the model.  The regularization parameter causes this to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there anything peculiar that you observe about the coefficients Ridge regression assigns to the first three predictors? Do you observe the same with Lasso regression? Give an explanation for your observation.\n",
    "\n",
    "The first three coefficients are the same in the Ridge regression, while the Lasso coefficient has a higher first coefficient.  I cannot give an explanation right now, I'll come back to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Cross-validation and Bootstrapping\n",
    "In this problem, you will work with an expanded version of the automobile pricing data set you analyzed in Homework 3. The data set is contained ``dataset_2.txt``, with 26 attribues (i.e. predictors) for each automobile and corresponding prices. \n",
    "\n",
    "### Part(a): Encode categorical attributes and fill missing values\n",
    "Identify the categorical attributes in the data. Replace their values with the one-hot binary encoding. You may do this using the `get_dummies()` function in `pandas`. If you do this task correctly, you should get a total of 69 predictors after the encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horsepower</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>symboling</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>make</th>\n",
       "      <th>fuel-type</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>num-of-doors</th>\n",
       "      <th>body-style</th>\n",
       "      <th>drive-wheels</th>\n",
       "      <th>...</th>\n",
       "      <th>engine-type</th>\n",
       "      <th>num-of-cylinders</th>\n",
       "      <th>engine-size</th>\n",
       "      <th>fuel-system</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.232558</td>\n",
       "      <td>peugot</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>wagon</td>\n",
       "      <td>rwd</td>\n",
       "      <td>...</td>\n",
       "      <td>l</td>\n",
       "      <td>four</td>\n",
       "      <td>120.0</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.19</td>\n",
       "      <td>8.4</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>toyota</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hardtop</td>\n",
       "      <td>rwd</td>\n",
       "      <td>...</td>\n",
       "      <td>ohc</td>\n",
       "      <td>four</td>\n",
       "      <td>146.0</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9.3</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>bmw</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>...</td>\n",
       "      <td>ohc</td>\n",
       "      <td>six</td>\n",
       "      <td>164.0</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.19</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.232558</td>\n",
       "      <td>mercedes-benz</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>rwd</td>\n",
       "      <td>...</td>\n",
       "      <td>ohcv</td>\n",
       "      <td>eight</td>\n",
       "      <td>308.0</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.35</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>subaru</td>\n",
       "      <td>gas</td>\n",
       "      <td>turbo</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>...</td>\n",
       "      <td>ohcf</td>\n",
       "      <td>four</td>\n",
       "      <td>108.0</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.64</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11259.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   horsepower  highway-mpg symboling  normalized-losses           make  \\\n",
       "0        95.0         24.0         0         120.232558         peugot   \n",
       "1       116.0         30.0         2         134.000000         toyota   \n",
       "2       121.0         28.0         0         188.000000            bmw   \n",
       "3       184.0         16.0         0         120.232558  mercedes-benz   \n",
       "4       111.0         29.0         0         102.000000         subaru   \n",
       "\n",
       "  fuel-type aspiration num-of-doors body-style drive-wheels   ...     \\\n",
       "0       gas        std         four      wagon          rwd   ...      \n",
       "1       gas        std          two    hardtop          rwd   ...      \n",
       "2       gas        std          two      sedan          rwd   ...      \n",
       "3       gas        std         four      sedan          rwd   ...      \n",
       "4       gas      turbo         four      sedan          4wd   ...      \n",
       "\n",
       "  engine-type  num-of-cylinders  engine-size  fuel-system  bore  stroke  \\\n",
       "0           l              four        120.0         mpfi  3.46    2.19   \n",
       "1         ohc              four        146.0         mpfi  3.62    3.50   \n",
       "2         ohc               six        164.0         mpfi  3.31    3.19   \n",
       "3        ohcv             eight        308.0         mpfi  3.80    3.35   \n",
       "4        ohcf              four        108.0         mpfi  3.62    2.64   \n",
       "\n",
       "  compression-ratio peak-rpm  city-mpg    price  \n",
       "0               8.4   5000.0      19.0  16695.0  \n",
       "1               9.3   4800.0      24.0  11199.0  \n",
       "2               9.0   4250.0      21.0  20970.0  \n",
       "3               8.0   4500.0      14.0  40960.0  \n",
       "4               7.7   4800.0      24.0  11259.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data2=np.loadtxt('./datasets/dataset_2.txt',delimiter=',',skiprows=1,index=1)\n",
    "data2 = pd.read_csv('./datasets/dataset_2.txt', sep=',')\n",
    "data2['symboling']=data2['symboling'].astype(object)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.]]\n",
      "[[ 0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 0.  0.  0.  0.  1.]]\n",
      "[[ 0.  0.  1.]]\n",
      "[[ 1.  0.]]\n",
      "[[ 0.  1.  0.  0.  0.]]\n",
      "[[ 0.  0.  1.  0.]]\n",
      "[[ 0.  0.  0.  1.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horsepower</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>-2.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.09604519774</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>audi</th>\n",
       "      <th>...</th>\n",
       "      <th>1bbl</th>\n",
       "      <th>2bbl</th>\n",
       "      <th>idi</th>\n",
       "      <th>mpfi</th>\n",
       "      <th>spdi</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.232558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.19</td>\n",
       "      <td>8.4</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9.3</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.19</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.232558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.35</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.64</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   horsepower  highway-mpg  -2.0  0.0  1.0  1.09604519774  2.0  3.0  \\\n",
       "0        95.0         24.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "1       116.0         30.0   0.0  0.0  0.0            0.0  1.0  0.0   \n",
       "2       121.0         28.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "3       184.0         16.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "4       111.0         29.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "\n",
       "   normalized-losses  audi    ...     1bbl  2bbl  idi  mpfi  spdi  bore  \\\n",
       "0         120.232558   0.0    ...      0.0   0.0  0.0   1.0   0.0  3.46   \n",
       "1         134.000000   0.0    ...      0.0   0.0  0.0   1.0   0.0  3.62   \n",
       "2         188.000000   0.0    ...      0.0   0.0  0.0   1.0   0.0  3.31   \n",
       "3         120.232558   0.0    ...      0.0   0.0  0.0   1.0   0.0  3.80   \n",
       "4         102.000000   0.0    ...      0.0   0.0  0.0   1.0   0.0  3.62   \n",
       "\n",
       "   stroke  compression-ratio  peak-rpm  city-mpg  \n",
       "0    2.19                8.4    5000.0      19.0  \n",
       "1    3.50                9.3    4800.0      24.0  \n",
       "2    3.19                9.0    4250.0      21.0  \n",
       "3    3.35                8.0    4500.0      14.0  \n",
       "4    2.64                7.7    4800.0      24.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate predictors and response\n",
    "x_df = data2.iloc[:,:-1]\n",
    "y_df = data2.iloc[:,-1]\n",
    "\n",
    "d = np.shape(x_df)[1] # no. of attributes\n",
    "\n",
    "# Create a new data frame\n",
    "\n",
    "x_df_expanded = pd.DataFrame() \n",
    "\n",
    "# Need to make symboling column datatype equal object\n",
    "\n",
    "# x_df['symboling'].dtype='object'\n",
    "\n",
    "# Iterate over all attributes\n",
    "for column in x_df.columns:\n",
    "    # Check if attribute is not categorical: either dtype is not object\n",
    "    if(x_df[column].dtype != np.dtype('object')):\n",
    "        x_df_expanded = pd.concat([x_df_expanded, x_df[column]], axis=1)\n",
    "    else:\n",
    "        # otherwise: use one-hot encoding\n",
    "        encoding = pd.get_dummies(x_df[column])  # Convert categorical variable into dummy/indicator variables\n",
    "        # append expanded attribute to data frame\n",
    "        x_df_expanded = pd.concat([x_df_expanded, encoding], axis=1)\n",
    "        print encoding.iloc[:1].values\n",
    "x_df_expanded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Apply regular linear regression\n",
    "- Split the data set into train and test sets, with the first 25% of the data for training and the remaining for testing.  \n",
    "\n",
    "\n",
    "- Use regular linear regression to fit a model to the training set and evaluate the R^2 score of the fitted model on both the training and test sets. What do you observe about these values?\n",
    "\n",
    "\n",
    "- You had seen in class that the R^2 value of a least-squares fit to a data set would lie between 0 and 1. Is this true for the test R^2 values reported above? If not, give a reason for why this is the case.\n",
    "\n",
    "\n",
    "- Is there a need for regularization while fitting a linear model to this data set?\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a linear regression model and evaluate the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert data frame to array\n",
    "x = x_df_expanded.values\n",
    "y = y_df.values\n",
    "\n",
    "# ii. SPLIT TRAIN AND TEST SETS\n",
    "# No. of training points\n",
    "n = x.shape[0]\n",
    "n_train = int(np.round(n*0.25))\n",
    "\n",
    "# First 25% train, remaining test\n",
    "x_train = x[0:n_train,:]\n",
    "y_train = y[0:n_train]\n",
    "x_test = x[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5.97570431e+00  -8.49551804e+02   3.86450777e+02  -1.35984895e+00\n",
      "   9.51444955e+02   1.02090360e+04  -3.72919877e+03  -4.12332278e+03\n",
      "  -1.46380940e+03  -1.29687529e+01   4.30511034e+03  -3.75371201e-09\n",
      "  -7.06376124e+03   1.70227507e+03  -8.10567401e+02  -6.17867609e+03\n",
      "   4.94827916e+03   3.24504867e-11   2.83242638e+03  -5.48839876e+03\n",
      "  -2.34301724e+03   2.26217254e+03   4.00297602e+03   7.05076002e+03\n",
      "  -7.05076002e+03   7.14953882e+02  -7.14953882e+02   6.12600994e+02\n",
      "  -6.12600994e+02   4.34283720e-11   6.97272527e+02   3.03834848e+02\n",
      "   5.79075610e+02  -1.58018298e+03   1.02163207e+03   1.71200022e+02\n",
      "  -1.19283210e+03  -4.94827916e+03   4.94827916e+03  -3.01687572e+02\n",
      "   1.07398496e+02   3.63128282e+02  -1.46283337e+03   9.99366391e+00\n",
      "  -2.03789000e+03  -8.10567401e+02  -7.65234878e+02  -5.40119594e+02\n",
      "   4.15381188e+03   4.15381188e+03   1.10274342e+03  -2.94868028e+03\n",
      "  -2.30787501e+03   1.67550652e+01  -1.46380940e+03  -2.50845707e+03\n",
      "   7.05076002e+03  -1.39917235e+03  -1.67932119e+03  -2.75364625e+03\n",
      "  -4.73714723e+03  -1.68913463e+03   1.57445262e-01   9.94554819e+02] [ -5.97570431e+00  -8.49551804e+02   3.86450777e+02  -1.35984895e+00\n",
      "   9.51444955e+02   1.02090360e+04  -3.72919877e+03  -4.12332278e+03\n",
      "  -1.46380940e+03  -1.29687529e+01   4.30511034e+03  -3.75371201e-09\n",
      "  -7.06376124e+03   1.70227507e+03  -8.10567401e+02  -6.17867609e+03\n",
      "   4.94827916e+03   3.24504867e-11   2.83242638e+03  -5.48839876e+03\n",
      "  -2.34301724e+03   2.26217254e+03   4.00297602e+03   7.05076002e+03\n",
      "  -7.05076002e+03   7.14953882e+02  -7.14953882e+02   6.12600994e+02\n",
      "  -6.12600994e+02   4.34283720e-11   6.97272527e+02   3.03834848e+02\n",
      "   5.79075610e+02  -1.58018298e+03   1.02163207e+03   1.71200022e+02\n",
      "  -1.19283210e+03  -4.94827916e+03   4.94827916e+03  -3.01687572e+02\n",
      "   1.07398496e+02   3.63128282e+02  -1.46283337e+03   9.99366391e+00\n",
      "  -2.03789000e+03  -8.10567401e+02  -7.65234878e+02  -5.40119594e+02\n",
      "   4.15381188e+03   4.15381188e+03   1.10274342e+03  -2.94868028e+03\n",
      "  -2.30787501e+03   1.67550652e+01  -1.46380940e+03  -2.50845707e+03\n",
      "   7.05076002e+03  -1.39917235e+03  -1.67932119e+03  -2.75364625e+03\n",
      "  -4.73714723e+03  -1.68913463e+03   1.57445262e-01   9.94554819e+02] 107996.545865 107996.545865\n",
      "<bound method LinearRegression.score of LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)> <bound method LinearRegression.score of LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)>\n"
     ]
    }
   ],
   "source": [
    "# Fit sklearn multiple linear regression model\n",
    "reg = Lin_Reg()\n",
    "reg.fit(x_train, y_train)\n",
    "    \n",
    "# Get coefficients from fitted model\n",
    "w = reg.coef_\n",
    "c = reg.intercept_\n",
    "    \n",
    "# Fit sklearn multiple linear regression model\n",
    "regtest = Lin_Reg()\n",
    "regtest.fit(x_test, y_test)\n",
    "    \n",
    "# Get coefficients from fitted model\n",
    "wtest = reg.coef_\n",
    "ctest = reg.intercept_\n",
    "    \n",
    "print w,wtest,c,ctest\n",
    "\n",
    "regscore=reg.score\n",
    "regtestscore=regtest.score\n",
    "print regscore,regtestscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Ridge regression\n",
    "\n",
    "- Apply Ridge regression on the training set for different values of the regularization parameter $\\lambda$ in the range $\\{10^{-7}, 10^{-6}, \\ldots, 10^7\\}$. Evaluate the R^2 score for the models you obtain on both the train and test sets. Plot both values as a function of $\\lambda$. \n",
    "\n",
    "\n",
    "- Explain the relationship between the regularization parameter and the training and test R^2 scores.\n",
    "\n",
    "\n",
    "- How does the best test R^2 value obtained using Ridge regression compare with that of plain linear regression? Explain.\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a ridge regression model and evaluate the fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d): Tune regularization parameter using cross-validation and bootstrapping\n",
    "-  Evaluate the performance of the Ridge regression for different regularization parameters $\\lambda$ using 5-fold cross validation **or** bootstrapping on the training set. \n",
    "\n",
    "    - Plot the cross-validation (CV) or bootstrapping R^2 score as a function of $\\lambda$. \n",
    "    \n",
    "    - How closely does the CV score or bootstrapping score match the R^2 score on the test set? Does the model with lowest CV score or bootstrapping score correspond to the one with maximum R^2 on the test set?\n",
    "    \n",
    "    - Does the model chosen by CV or bootstrapping perform better than plain linear regression?\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a linear regression model and evaluate the fits. You may also use `kFold` from `sklearn.cross_validation`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Ridge regression *via* ordinary least-squares regression\n",
    "\n",
    "We present an approach to implement Ridge regression using oridinary least-squares regression. Given a matrix of responses $\\mathbf{X} \\in \\mathbb{R}^{n\\times p}$ and response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$, one can implement Ridge regression with regularization parameter $\\lambda$ as follows:\n",
    "\n",
    "- Augment the matrix of predictors $\\mathbf{X}$ with $p$ new rows containing the scaled identity matrix $\\sqrt{\\lambda}\\mathbf{I} \\in \\mathbb{R}^{p \\times p}$, i.e.\n",
    "$$\\overline{\\mathbf{X}} \\,=\\, \n",
    "\\begin{bmatrix}\n",
    "X_{11} & \\ldots & X_{1p}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "X_{n1} & \\ldots & X_{np}\\\\\n",
    "\\sqrt{\\lambda} & \\ldots & 0\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "0 & \\ldots & \\sqrt{\\lambda}\n",
    "\\end{bmatrix}\n",
    "\\,\\in\\,\n",
    "\\mathbb{R}^{(n+p)\\times p}\n",
    ".\n",
    "$$\n",
    "\n",
    "\n",
    "- Augment the response vector $\\mathbf{y}$ with a column of $p$ zeros, i.e.\n",
    "$$\n",
    "\\overline{\\mathbf{y}} \\,=\\, \n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\,\\in\\,\n",
    "\\mathbb{R}^{n+p}.\n",
    "$$\n",
    "\n",
    "\n",
    "- Apply ordinary least-squares regression on the augmented data set $(\\overline{\\mathbf{X}}, \\overline{\\mathbf{y}})$.\n",
    "\n",
    "### Part (a): Show the proposed approach implements Ridge regression\n",
    "Show that the approach proposed above implements Ridge regression with parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Debug our implementation of ridge regression\n",
    "You're a grader for CS109A, the following is an implemention of Ridge regression (via the above approach) submitted by a student. The dataset is ``dataset_3.txt``. The regression model is fitted to a training set, and the R^2 scores of the fitted model on the training and test sets are plotted as a function of the regularization parameter. Grade this solution according to the following rubric (each category is equally weighted): \n",
    "\n",
    "- correctness\n",
    "\n",
    "- interpretation (if applicable)\n",
    "\n",
    "- code/algorithm design\n",
    "\n",
    "- presentation\n",
    "\n",
    "In addition to providing an holistic grade (between 0 to 5), provide a corrected version of this code that is submission quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1149841d0>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114906290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit\n",
    "def ridge(x_train, y_train, reg_param):\n",
    "    n=np.shape(x_train)[0]\n",
    "    x_train=np.concatenate((x_train,reg_param*np.identity(n)),axis=1)\n",
    "    y_train_=np.zeros((n+np.shape(x_train)[1],1))\n",
    "    for c in range(n):\n",
    "        y_train_[c]= y_train[c]\n",
    "    import sklearn\n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "    model.fit(x_train,y_train.reshape(-1,1))\n",
    "    return model\n",
    "\n",
    "# Score\n",
    "def score(m,x_test,y_test, reg_param):\n",
    "    n=np.shape(x_train)[0]\n",
    "    x_test=np.concatenate((x_test,reg_param*np.identity(n)),axis=1)\n",
    "    y_test_=np.zeros((n+np.shape(x_test)[1],1))\n",
    "    for c in range(n):\n",
    "        y_test_[c]= y_test[c]\n",
    "    return m.score(x_test,y_test.reshape(-1,1))\n",
    "\n",
    "# Load\n",
    "data = np.loadtxt('datasets/dataset_3.txt', delimiter=',')\n",
    "n = data.shape[0]\n",
    "n = int(np.round(n*0.5))\n",
    "x_train = data[0:n,0:100]\n",
    "y_train = data[0:n,100]\n",
    "x_test = data[n:2*n,0:100]\n",
    "y_test = data[n:2*n,100]\n",
    "\n",
    "# Params\n",
    "a=np.zeros(5)\n",
    "for i in range(-2,2):\n",
    "    a[i+2]=10**i\n",
    "\n",
    "# Iterate\n",
    "rstr =np.zeros(5)\n",
    "rsts =np.zeros(5)\n",
    "for j in range(0,5):    \n",
    "    m =ridge(x_train,y_train,a[i])\n",
    "    rstr[j]=score(m,x_train,y_train,a[j])\n",
    "    rsts[i]=score(m,x_test,y_test,a[i])\n",
    "\n",
    "# Plot\n",
    "plt.plot(a,rstr)\n",
    "plt.plot(a,rsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Predicting Outcome of a Fund-raising Campaign\n",
    "You are provided a data set containing details of mail sent to 95,412 potential donors for a fund-raising campaign of a not-for-profit organization. This data set also contains the amount donated by each donor. The task is to build a model that can estimate the amount that a donor would donate using his/her attributes. The data is contained in the file `dataset_4.txt`. Each row contains 376 attributes for a donor, followed by the donation amount.\n",
    "\n",
    "**Note**: For additional information about the attributes used, please look up the file `dataset_4_description.txt`. This files also contains details of attributes that have been omitted from the data set.\n",
    "\n",
    "### Part (a): Fit regression model\n",
    "Build a suitable model to predict the donation amount. How good is your model? \n",
    "\n",
    "\n",
    "### Part (b): Evaluate the total profit of the fitted model\n",
    "Suppose you are told that the cost of mailing the donor is \\$7. Use your model to maximize profit. Implement, explain and rigorously justify your strategy. How does your strategry compare with blanket mailing everyone.\n",
    "\n",
    "### Part (c): Further Discussion\n",
    "In hindsight, thoroughly discuss the appropriatenes of using a regression model for this dataset (you must at least address the suitability with respect to profit maximization and model assumptions). Rigorously justify your reasoning. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
